# Expand KubernetesTemporal Spec with Dynamic Configuration and Server Settings

## Problem

The current `KubernetesTemporalSpec` exposes only basic configuration for deploying Temporal on Kubernetes. Critical server-side settings—particularly **dynamic configuration**—are not exposed, preventing users from tuning Temporal's behavior for their specific workloads.

One concrete example: Temporal enforces workflow history size limits (default ~50MB or ~51,200 events). When workflows exceed these limits, Temporal terminates them with:

```json
{
  "type": "workflowExecutionTerminatedEventAttributes",
  "reason": "Workflow history size exceeds limit.",
  "identity": "history-service"
}
```

Users currently have no way to adjust these limits through the `KubernetesTemporalSpec`. They must either:
1. Manually patch the Helm release after deployment
2. Fork and customize the Pulumi/Terraform modules
3. Redesign their workflows to use `ContinueAsNew` pattern

This violates Project Planton's "80/20 principle"—these are essential configuration options that a significant percentage of production users need.

## Proposed Solution

Expand `KubernetesTemporalSpec` to include:

### 1. Dynamic Configuration Support

Add a `dynamic_config` field that maps to Temporal's Helm chart `server.config.dynamicConfigValues`:

```protobuf
message KubernetesTemporalSpec {
  // ... existing fields ...
  
  // Dynamic configuration values for Temporal server
  // These settings control runtime behavior without requiring server restart
  KubernetesTemporalDynamicConfig dynamic_config = 12;
}

message KubernetesTemporalDynamicConfig {
  // Maximum size in bytes for workflow execution history
  // Default: 52428800 (50 MB). Increase for complex workflows.
  optional int64 history_size_limit_error = 1 [(org.project_planton.shared.options.default) = "52428800"];
  
  // Maximum number of events in workflow execution history
  // Default: 51200. Increase for workflows with many activities/signals.
  optional int64 history_count_limit_error = 2 [(org.project_planton.shared.options.default) = "51200"];
  
  // Warning threshold for history size (percentage of error limit)
  // Default: 10485760 (10 MB, ~20% of error limit)
  optional int64 history_size_limit_warn = 3;
  
  // Warning threshold for history count
  // Default: 10240 (~20% of error limit)
  optional int64 history_count_limit_warn = 4;
  
  // Maximum concurrent workflow task pollers per worker
  optional int32 max_concurrent_workflow_task_pollers = 5;
  
  // Maximum concurrent activity task pollers per worker
  optional int32 max_concurrent_activity_task_pollers = 6;
}
```

### 2. History Shards Configuration

The number of history shards (`numHistoryShards`) is an **immutable Day 0 decision** that affects cluster scalability. Currently not exposed:

```protobuf
message KubernetesTemporalSpec {
  // ... existing fields ...
  
  // Number of history shards. This is an IMMUTABLE setting that must be decided at creation time.
  // Higher values enable better parallelism but require more resources.
  // Default: 512 (safe for most production workloads)
  // WARNING: Cannot be changed after initial deployment without data migration.
  optional int32 num_history_shards = 13 [(org.project_planton.shared.options.default) = "512"];
}
```

### 3. Service Resource Configuration

Allow configuring resources for individual Temporal services (frontend, history, matching, worker):

```protobuf
message KubernetesTemporalServiceResources {
  // Frontend service configuration
  KubernetesTemporalServiceConfig frontend = 1;
  
  // History service configuration (most resource-intensive)
  KubernetesTemporalServiceConfig history = 2;
  
  // Matching service configuration
  KubernetesTemporalServiceConfig matching = 3;
  
  // Internal worker service configuration
  KubernetesTemporalServiceConfig worker = 4;
}

message KubernetesTemporalServiceConfig {
  // Number of replicas
  optional int32 replicas = 1 [(org.project_planton.shared.options.default) = "1"];
  
  // Container resources
  org.project_planton.provider.kubernetes.ContainerResources resources = 2;
}
```

## Impact

### Who Benefits

- **Platform engineers** deploying Temporal for high-throughput workflow systems
- **Backend engineers** running long-running or complex orchestration workflows
- **Operations teams** needing to tune Temporal for specific workload patterns
- **Any user** hitting default limits in production

### Without This Feature

Users must:
- Manually patch Helm releases after Project Planton deployment
- Fork modules to add custom configuration
- Over-provision resources to avoid hitting limits
- Redesign workflows to work around server defaults

### With This Feature

Users can:
- Configure history limits directly in their YAML manifest
- Set appropriate shard counts at deployment time
- Tune service resources based on workload needs
- Follow Project Planton's standard validation-first workflow

## Acceptance Criteria

- [ ] `KubernetesTemporalSpec` expanded with `dynamic_config` message
- [ ] `KubernetesTemporalSpec` expanded with `num_history_shards` field
- [ ] `KubernetesTemporalSpec` expanded with service-level resource configuration
- [ ] Validation rules added for new fields (min/max bounds, immutability warnings)
- [ ] Pulumi module updated to pass dynamic config to Helm chart
- [ ] Terraform module updated with feature parity
- [ ] Documentation updated with new configuration options
- [ ] Examples updated to show common tuning scenarios
- [ ] Proto stubs regenerated and published

## Implementation Notes

### Helm Chart Mapping

The Temporal Helm chart accepts dynamic config via:

```yaml
server:
  config:
    dynamicConfigValues:
      limit.historySize.error:
        - value: 104857600  # 100 MB
      limit.historyCount.error:
        - value: 102400     # 100K events
```

The Pulumi module should transform `KubernetesTemporalDynamicConfig` into this nested structure.

### Validation Considerations

- `num_history_shards` should have a warning annotation about immutability
- History limits should have minimum bounds (can't set to 0 or negative)
- Resource configurations should follow standard Kubernetes resource patterns

### Documentation Priority

These are advanced settings. Document:
1. When to adjust (symptoms of hitting limits)
2. Trade-offs (higher limits = more memory usage)
3. The `ContinueAsNew` pattern as an alternative to increasing limits

## Related

- Temporal documentation on [history limits](https://docs.temporal.io/workflows#limits)
- Temporal Helm chart [values.yaml](https://github.com/temporalio/helm-charts)
- Project Planton [80/20 design philosophy](https://github.com/plantonhq/project-planton/blob/main/architecture/README.md#the-8020-principle)

## Priority

**Medium-High** — This blocks production use cases where workflows exceed default limits. The workarounds (manual Helm patching, module forking) defeat the purpose of using Project Planton.

