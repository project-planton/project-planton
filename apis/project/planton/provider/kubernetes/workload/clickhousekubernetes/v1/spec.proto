syntax = "proto3";

package project.planton.provider.kubernetes.workload.clickhousekubernetes.v1;

import "buf/validate/validate.proto";
import "project/planton/shared/kubernetes/kubernetes.proto";
import "project/planton/shared/kubernetes/options.proto";
import "project/planton/shared/options/options.proto";
import "google/protobuf/descriptor.proto";

extend google.protobuf.FieldOptions {
  ClickHouseKubernetesContainer default_container = 519001;
}

/**
 * **ClickHouseKubernetesSpec** defines the configuration for deploying ClickHouse on a Kubernetes cluster.
 * This message specifies the parameters needed to create and manage a ClickHouse deployment within a Kubernetes environment.
 * It includes cluster name, container specifications, ingress settings, and cluster configuration options.
 * 
 * The deployment uses the Altinity ClickHouse Operator, which provides production-grade ClickHouse cluster management
 * with features like automated upgrades, scaling, and ZooKeeper coordination.
 */
message ClickHouseKubernetesSpec {
  /**
   * The name of the ClickHouse cluster.
   * This is used as the identifier for the ClickHouseInstallation custom resource.
   * Must be a valid DNS subdomain name (lowercase alphanumeric with hyphens).
   * Defaults to the resource metadata name if not specified.
   */
  string cluster_name = 1 [
    (buf.validate.field).string.pattern = "^[a-z0-9]([-a-z0-9]*[a-z0-9])?$"
  ];

  // The container specifications for the ClickHouse deployment.
  ClickHouseKubernetesContainer container = 2 [
    (default_container) = {
      replicas: 1,
      resources:  {
        limits {
          cpu: "2000m"
          memory: "4Gi"
        },
        requests {
          cpu: "500m"
          memory: "1Gi"
        }
      },
      is_persistence_enabled: true,
      disk_size: "50Gi"
    }
  ];

  // The ingress configuration for the ClickHouse deployment.
  project.planton.shared.kubernetes.IngressSpec ingress = 3;

  // The cluster configuration for ClickHouse sharding and replication.
  ClickHouseKubernetesClusterConfig cluster = 4;

  /**
   * The ClickHouse version to deploy (e.g., "24.3", "23.8").
   * If not specified, the operator's default stable version will be used.
   * It's recommended to specify a version for production deployments to ensure consistency.
   */
  string version = 5;

  /**
   * Coordination configuration for cluster operations.
   * Required when cluster.is_enabled = true.
   * 
   * Recommended: Leave unspecified to use auto-managed ClickHouse Keeper (default).
   * This is more efficient than ZooKeeper and easier to manage.
   * 
   * Advanced: Configure external Keeper or ZooKeeper for shared infrastructure scenarios.
   */
  ClickHouseKubernetesCoordinationConfig coordination = 6;

  /**
   * ZooKeeper configuration for cluster coordination.
   * 
   * DEPRECATED: Use 'coordination' field instead.
   * This field is kept for backward compatibility and will be removed in v2.
   * 
   * If both 'coordination' and 'zookeeper' are specified, 'coordination' takes precedence.
   */
  ClickHouseKubernetesZookeeperConfig zookeeper = 7 [deprecated = true];

  /**
   * Logging configuration for ClickHouse server.
   * Controls the verbosity of ClickHouse server logs.
   * 
   * Required field. Defaults to 'information' level (recommended for production).
   * Use 'debug' or 'trace' levels only for troubleshooting as they generate significant log volume.
   */
  ClickHouseKubernetesLoggingConfig logging = 8 [
    (buf.validate.field).required = true
  ];
}

/**
 * **ClickHouseKubernetesContainer** specifies the container configuration for the ClickHouse application.
 * It includes resource allocations, data persistence options, and disk size.
 * Proper configuration ensures optimal performance and data reliability for your ClickHouse deployment.
 */
message ClickHouseKubernetesContainer {
  /**
   * The number of ClickHouse replicas (pods) to deploy.
   * For non-clustered deployments, this is the total number of ClickHouse pods.
   * For clustered deployments, this value is ignored in favor of shard_count * replica_count.
   * Must be at least 1.
   */
  int32 replicas = 1 [
    (buf.validate.field).int32.gte = 1
  ];

  // The CPU and memory resources allocated to each ClickHouse container.
  project.planton.shared.kubernetes.ContainerResources resources = 2;

  /**
   * A flag to enable or disable data persistence for ClickHouse.
   * When enabled, data is persisted to a storage volume, allowing data to survive pod restarts.
   * Defaults to `true`. Strongly recommended for production use.
   */
  bool is_persistence_enabled = 3;

  /**
   * The size of the persistent volume attached to each ClickHouse pod (e.g., "50Gi", "100Gi").
   * If the client does not provide a value, a default value is configured.
   * This attribute is ignored when persistence is not enabled.
   * **Note:** This value cannot be easily modified after creation due to Kubernetes limitations.
   * Plan for growth and allocate sufficient storage initially.
   */
  string disk_size = 4;

  option (buf.validate.message).cel = {
    id: "spec.container.disk_size.required",
    expression: "((!this.is_persistence_enabled && (size(this.disk_size) == 0 || this.disk_size == '')) || (this.is_persistence_enabled && size(this.disk_size) > 0 && this.disk_size.matches('^\\\\d+(\\\\.\\\\d+)?\\\\s?(Ki|Mi|Gi|Ti|Pi|Ei|K|M|G|T|P|E)$')))"
    message: "Disk size is required and must match the format if persistence is enabled"
  };
}

/**
 * **ClickHouseKubernetesClusterConfig** defines the clustering configuration for ClickHouse.
 * This includes settings for sharding and replication to enable distributed ClickHouse deployments.
 * 
 * Clustering provides horizontal scaling and high availability. When clustering is enabled:
 * - Data is distributed across shards for parallel processing
 * - Each shard can have multiple replicas for redundancy
 * - ZooKeeper is used for cluster coordination (automatically managed by the operator)
 */
message ClickHouseKubernetesClusterConfig {
  /**
   * A flag to enable or disable clustering mode for ClickHouse.
   * When enabled, ClickHouse will be deployed in a distributed cluster configuration with sharding and replication.
   * When disabled, a single standalone ClickHouse instance is deployed.
   * Defaults to `false` (standalone mode suitable for development and small workloads).
   */
  bool is_enabled = 1;

  /**
   * The number of shards in the ClickHouse cluster.
   * Sharding distributes data across multiple nodes for horizontal scaling and improved query performance.
   * Each shard processes queries in parallel, allowing for massive throughput.
   * This value is ignored if clustering is not enabled.
   * Typical values: 2-8 for most use cases, can go higher for very large deployments.
   */
  int32 shard_count = 2;

  /**
   * The number of replicas for each shard.
   * Replication provides data redundancy and high availability.
   * If one replica fails, queries can be served by other replicas in the same shard.
   * This value is ignored if clustering is not enabled.
   * Typical values: 2-3 (more than 3 replicas is rarely needed).
   */
  int32 replica_count = 3;

  option (buf.validate.message).cel = {
    id: "spec.cluster.counts_when_enabled",
    expression: "!this.is_enabled || (this.shard_count >= 1 && this.replica_count >= 1)"
    message: "Shard count and replica count must be at least 1 when clustering is enabled"
  };
}

/**
 * **ClickHouseKubernetesCoordinationConfig** defines coordination service configuration for ClickHouse cluster.
 * ClickHouse requires coordination for distributed operations (DDL execution, replication management).
 * 
 * For most use cases, auto-managed ClickHouse Keeper (default) is recommended.
 * It's more efficient than ZooKeeper (75% less resources) and managed by the same operator.
 * 
 * Use external coordination when:
 * - Sharing infrastructure across multiple ClickHouse clusters
 * - Using existing ZooKeeper for other services (Kafka, etc.)
 * - Migrating from ZooKeeper to ClickHouse Keeper
 */
message ClickHouseKubernetesCoordinationConfig {
  /**
   * Type of coordination service to use.
   */
  enum CoordinationType {
    // Unspecified defaults to 'keeper' (recommended for new deployments)
    unspecified = 0;
    
    // Auto-managed ClickHouse Keeper deployed by the operator.
    // Recommended: More efficient than ZooKeeper, easier to manage.
    // Creates a ClickHouseKeeperInstallation resource automatically.
    keeper = 1;
    
    // Use existing ClickHouse Keeper cluster.
    // For shared infrastructure or advanced scenarios.
    external_keeper = 2;
    
    // Use existing ZooKeeper cluster.
    // For legacy systems or shared ZooKeeper infrastructure (Kafka, etc.)
    external_zookeeper = 3;
  }

  /**
   * Type of coordination service to use.
   * Defaults to 'keeper' (auto-managed ClickHouse Keeper).
   */
  CoordinationType type = 1;
  
  /**
   * Configuration for auto-managed ClickHouse Keeper.
   * Only used when type = 'keeper'.
   * If not specified, sensible defaults are used (1 replica for dev, 3 for prod).
   */
  ClickHouseKubernetesKeeperConfig keeper_config = 2;
  
  /**
   * Configuration for external coordination service.
   * Only used when type = 'external_keeper' or 'external_zookeeper'.
   * Must specify at least one node.
   */
  ClickHouseKubernetesExternalCoordinationConfig external_config = 3;
}

/**
 * **ClickHouseKubernetesKeeperConfig** defines configuration for auto-managed ClickHouse Keeper.
 * The operator creates a ClickHouseKeeperInstallation resource with these settings.
 * 
 * ClickHouse Keeper is a ZooKeeper alternative written in C++ specifically for ClickHouse.
 * Benefits: 75% less CPU/memory usage, no JVM overhead, protocol-compatible with ZooKeeper.
 */
message ClickHouseKubernetesKeeperConfig {
  /**
   * Number of ClickHouse Keeper replicas.
   * Must be an odd number for quorum (1, 3, or 5).
   * 
   * Recommendations:
   * - Development/Testing: 1 (no fault tolerance)
   * - Production: 3 (survives 1 node failure)
   * - Large Production: 5 (survives 2 node failures)
   * 
   * Defaults to 1 if not specified.
   */
  int32 replicas = 1 [
    (buf.validate.field).int32 = {in: [1, 3, 5]}
  ];
  
  /**
   * Resources for each Keeper pod.
   * 
   * Recommended defaults (applied if not specified):
   * - Requests: 100m CPU, 256Mi memory
   * - Limits: 500m CPU, 1Gi memory
   * 
   * ClickHouse Keeper is very efficient; these defaults work for most deployments.
   */
  project.planton.shared.kubernetes.ContainerResources resources = 2;
  
  /**
   * Persistent volume size for each Keeper pod.
   * Defaults to "10Gi" if not specified.
   * 
   * Keeper stores coordination metadata (not data), so 10Gi is sufficient for most use cases.
   * Consider larger sizes (20-50Gi) for very large clusters (100+ nodes).
   */
  string disk_size = 3 [
    (buf.validate.field).string.pattern = "^\\d+(\\.\\d+)?\\s?(Ki|Mi|Gi|Ti|Pi|Ei|K|M|G|T|P|E)$"
  ];
}

/**
 * **ClickHouseKubernetesExternalCoordinationConfig** defines external coordination service configuration.
 * Use when connecting to existing ClickHouse Keeper or ZooKeeper infrastructure.
 * 
 * Common scenarios:
 * - Shared ZooKeeper used by Kafka, Solr, and ClickHouse
 * - Centrally managed Keeper infrastructure
 * - Multi-cluster coordination through shared Keeper
 */
message ClickHouseKubernetesExternalCoordinationConfig {
  /**
   * List of coordination service nodes in "host:port" format.
   * 
   * For production, specify all nodes in the ensemble for redundancy.
   * 
   * Examples:
   * - ClickHouse Keeper: ["keeper-prod:2181"]
   * - ClickHouse Keeper HA: ["keeper-0.keeper-svc:2181", "keeper-1.keeper-svc:2181", "keeper-2.keeper-svc:2181"]
   * - ZooKeeper: ["zk-0.zk.svc:2181", "zk-1.zk.svc:2181", "zk-2.zk.svc:2181"]
   * 
   * Port 2181 is the standard client port for both ZooKeeper and ClickHouse Keeper.
   */
  repeated string nodes = 1 [
    (buf.validate.field).repeated.min_items = 1
  ];
}

/**
 * **ClickHouseKubernetesLoggingConfig** defines logging configuration for ClickHouse server.
 * Controls the verbosity and behavior of ClickHouse server logs.
 * 
 * ClickHouse logging can significantly impact I/O performance and disk usage.
 * Choose the appropriate level based on your operational needs.
 */
message ClickHouseKubernetesLoggingConfig {
  /**
   * Log level for ClickHouse server.
   */
  enum LogLevel {
    // Information level: Logs errors, warnings, and important operational events.
    // Default and recommended for production environments.
    // Typical log volume: 5-10 MB/day per pod under normal load.
    information = 0;
    
    // Debug level: Logs detailed debugging information including query execution details.
    // Use for troubleshooting specific issues.
    // Warning: Generates 10-50x more logs than information level.
    // Typical log volume: 50-500 MB/day per pod.
    debug = 1;
    
    // Trace level: Logs extensive tracing information including internal operations.
    // Use only for deep troubleshooting or development.
    // Warning: Generates 50-100x more logs than information level.
    // Can impact performance due to I/O overhead.
    // Typical log volume: 500MB-5GB/day per pod.
    trace = 2;
  }

  /**
   * The log level for ClickHouse server logs.
   * Defaults to 'information' if not specified.
   * 
   * Production recommendation: Use 'information' for normal operations.
   * Temporarily switch to 'debug' or 'trace' when troubleshooting specific issues,
   * then revert to 'information' to avoid excessive log volume.
   */
  LogLevel level = 1;
}

/**
 * **ClickHouseKubernetesZookeeperConfig** defines the ZooKeeper configuration for ClickHouse cluster coordination.
 * 
 * DEPRECATED: This message is deprecated in favor of ClickHouseKubernetesCoordinationConfig.
 * Use the 'coordination' field in ClickHouseKubernetesSpec instead.
 * 
 * This is kept for backward compatibility and will be removed in v2.
 */
message ClickHouseKubernetesZookeeperConfig {
  /**
   * A flag to use external ZooKeeper instead of operator-managed ZooKeeper.
   * When false (default), the operator automatically provisions and manages ZooKeeper pods.
   * When true, you must provide external ZooKeeper nodes.
   */
  bool use_external = 1;

  /**
   * List of external ZooKeeper nodes in the format "host:port".
   * This is only used when use_external is true.
   * Example: ["zk-0.zk-headless.default.svc.cluster.local:2181", "zk-1.zk-headless.default.svc.cluster.local:2181"]
   */
  repeated string nodes = 2;

  option (buf.validate.message).cel = {
    id: "spec.zookeeper.nodes_when_external",
    expression: "!this.use_external || size(this.nodes) >= 1"
    message: "At least one ZooKeeper node must be specified when using external ZooKeeper"
  };
}
