syntax = "proto3";

package org.project_planton.provider.kubernetes.workload.kubernetessignoz.v1;

import "buf/validate/validate.proto";
import "google/protobuf/descriptor.proto";
import "org/project_planton/shared/kubernetes/kubernetes.proto";
import "org/project_planton/shared/options/options.proto";

extend google.protobuf.FieldOptions {
  KubernetesSignozContainer default_signoz_container = 560001;
  KubernetesSignozContainer default_otel_collector_container = 560002;
  KubernetesSignozClickhouseContainer default_clickhouse_container = 560003;
  KubernetesSignozZookeeperContainer default_zookeeper_container = 560004;
}

/**
 * **KubernetesSignozSpec** defines the configuration for deploying SigNoz observability platform on Kubernetes.
 * SigNoz is an OpenTelemetry-native platform that unifies logs, metrics, and traces into a single application.
 * This spec supports both self-managed and external ClickHouse database configurations, enabling flexible
 * deployment patterns from simple single-node installations to production-grade distributed clusters.
 */
message KubernetesSignozSpec {
  // The container specifications for the main SigNoz binary (UI, API server, Ruler, Alertmanager).
  KubernetesSignozContainer signoz_container = 1 [(default_signoz_container) = {
    replicas: 1
    resources: {
      limits: {
        cpu: "1000m"
        memory: "2Gi"
      }
      requests: {
        cpu: "200m"
        memory: "512Mi"
      }
    }
  }];

  // The container specifications for the OpenTelemetry Collector (data ingestion gateway).
  KubernetesSignozContainer otel_collector_container = 2 [(default_otel_collector_container) = {
    replicas: 2
    resources: {
      limits: {
        cpu: "2000m"
        memory: "4Gi"
      }
      requests: {
        cpu: "500m"
        memory: "1Gi"
      }
    }
  }];

  // The database configuration for SigNoz, supporting both self-managed and external ClickHouse.
  KubernetesSignozDatabaseConfig database = 3 [(buf.validate.field).required = true];

  // The ingress configuration for SigNoz UI and OpenTelemetry Collector endpoints.
  KubernetesSignozIngress ingress = 4;

  /**
   * A map of key-value pairs that provide additional customization options for the SigNoz Helm chart.
   * These values allow for further refinement of the deployment, such as setting environment variables,
   * configuring alerting integrations, or customizing retention policies.
   * For detailed information on available options, refer to the Helm chart documentation at:
   * https://github.com/SigNoz/charts
   */
  map<string, string> helm_values = 5;
}

/**
 * **KubernetesSignozContainer** specifies the container configuration for various SigNoz components.
 * It includes settings such as the number of replicas, container image, and resource allocations.
 */
message KubernetesSignozContainer {
  // The number of pods to deploy for this component.
  int32 replicas = 1 [(buf.validate.field).int32.gte = 1];

  // The CPU and memory resources allocated to the container.
  org.project_planton.shared.kubernetes.ContainerResources resources = 2;

  // The container image configuration (repository and tag).
  org.project_planton.shared.kubernetes.ContainerImage image = 3;
}

/**
 * **KubernetesSignozDatabaseConfig** defines the ClickHouse database configuration for SigNoz.
 * It supports two deployment modes:
 * 1. Self-managed: Deploy ClickHouse and Zookeeper within the Kubernetes cluster (default).
 * 2. External: Connect to an existing external ClickHouse instance.
 */
message KubernetesSignozDatabaseConfig {
  /**
   * Flag to enable using an external ClickHouse database.
   * When false (default), SigNoz will deploy and manage its own ClickHouse instance.
   * When true, the external_database field must be configured.
   */
  bool is_external = 1;

  /**
   * External ClickHouse database connection details.
   * This field is required when is_external is true and ignored when false.
   */
  KubernetesSignozExternalClickhouse external_database = 2;

  /**
   * Self-managed ClickHouse configuration.
   * This field is used when is_external is false and configures the in-cluster ClickHouse deployment.
   */
  KubernetesSignozManagedClickhouse managed_database = 3;

  option (buf.validate.message).cel = {
    id: "spec.database.external_required"
    expression: "!this.is_external || has(this.external_database)"
    message: "External database configuration is required when is_external is true"
  };
}

/**
 * **KubernetesSignozExternalClickhouse** defines connection parameters for an external ClickHouse instance.
 * This allows SigNoz to use a pre-existing ClickHouse database instead of deploying one within the cluster.
 */
message KubernetesSignozExternalClickhouse {
  // The hostname or endpoint of the external ClickHouse instance.
  string host = 1 [(buf.validate.field).required = true];

  // The HTTP port for ClickHouse (default is 8123).
  optional int32 http_port = 2 [
    (org.project_planton.shared.options.default) = "8123",
    (buf.validate.field).int32.gt = 0,
    (buf.validate.field).int32.lte = 65535
  ];

  // The TCP port for ClickHouse native protocol (default is 9000).
  optional int32 tcp_port = 3 [
    (org.project_planton.shared.options.default) = "9000",
    (buf.validate.field).int32.gt = 0,
    (buf.validate.field).int32.lte = 65535
  ];

  // The name of the distributed cluster in ClickHouse configuration.
  optional string cluster_name = 4 [(org.project_planton.shared.options.default) = "cluster"];

  // Whether to use secure (TLS) connection to ClickHouse.
  bool is_secure = 5;

  // The username for authenticating to ClickHouse.
  string username = 6 [(buf.validate.field).required = true];

  // The password for authenticating to ClickHouse.
  string password = 7 [(buf.validate.field).required = true];
}

/**
 * **KubernetesSignozManagedClickhouse** defines configuration for a self-managed ClickHouse deployment.
 * This supports both simple single-node deployments and production-grade distributed clusters with high availability.
 */
message KubernetesSignozManagedClickhouse {
  // The container specifications for ClickHouse.
  KubernetesSignozClickhouseContainer container = 1 [(default_clickhouse_container) = {
    replicas: 1
    resources: {
      limits: {
        cpu: "2000m"
        memory: "4Gi"
      }
      requests: {
        cpu: "500m"
        memory: "1Gi"
      }
    }
    persistence_enabled: true
    disk_size: "20Gi"
  }];

  // The cluster configuration for ClickHouse (sharding and replication).
  KubernetesSignozClickhouseCluster cluster = 2;

  // The Zookeeper configuration (required for distributed ClickHouse clusters).
  KubernetesSignozZookeeperConfig zookeeper = 3;
}

/**
 * **KubernetesSignozClickhouseContainer** specifies the container configuration for ClickHouse.
 * It includes replica count, resource allocations, and persistence settings.
 */
message KubernetesSignozClickhouseContainer {
  // The number of ClickHouse pods to deploy.
  int32 replicas = 1 [(buf.validate.field).int32.gte = 1];

  // The CPU and memory resources allocated to the ClickHouse container.
  org.project_planton.shared.kubernetes.ContainerResources resources = 2;

  // The container image configuration for ClickHouse.
  org.project_planton.shared.kubernetes.ContainerImage image = 3;

  /**
   * Flag to enable or disable data persistence for ClickHouse.
   * When enabled, data is persisted to a storage volume, allowing data to survive pod restarts.
   * Defaults to true.
   */
  bool persistence_enabled = 4;

  /**
   * The size of the persistent volume attached to each ClickHouse pod (e.g., "20Gi").
   * This attribute is ignored when persistence is not enabled.
   * Note: This value cannot be modified after creation due to Kubernetes StatefulSet limitations.
   */
  string disk_size = 5;

  option (buf.validate.message).cel = {
    id: "spec.clickhouse.container.disk_size.required"
    expression: "((!this.persistence_enabled && (size(this.disk_size) == 0 || this.disk_size == '')) || (this.persistence_enabled && size(this.disk_size) > 0 && this.disk_size.matches('^\\\\d+(\\\\.\\\\d+)?\\\\s?(Ki|Mi|Gi|Ti|Pi|Ei|K|M|G|T|P|E)$')))"
    message: "Disk size is required and must match the format if persistence is enabled"
  };
}

/**
 * **KubernetesSignozClickhouseCluster** defines the clustering configuration for ClickHouse.
 * Clustering enables distributed data storage and high availability through sharding and replication.
 * Note: Clustering requires Zookeeper to be configured.
 */
message KubernetesSignozClickhouseCluster {
  /**
   * Flag to enable or disable clustering mode for ClickHouse.
   * When enabled, ClickHouse will be deployed in a distributed cluster configuration.
   * Defaults to false.
   */
  bool is_enabled = 1;

  /**
   * The number of shards in the ClickHouse cluster.
   * Sharding distributes data across multiple nodes for horizontal scaling.
   * Recommended: 2 or more for production.
   * This value is ignored if clustering is not enabled.
   */
  int32 shard_count = 2;

  /**
   * The number of replicas for each shard.
   * Replication provides data redundancy and high availability.
   * Recommended: 2 for production.
   * This value is ignored if clustering is not enabled.
   */
  int32 replica_count = 3;

  option (buf.validate.message).cel = {
    id: "spec.clickhouse.cluster.counts_when_enabled"
    expression: "!this.is_enabled || (this.shard_count >= 1 && this.replica_count >= 1)"
    message: "Shard count and replica count must be at least 1 when clustering is enabled"
  };
}

/**
 * **KubernetesSignozZookeeperConfig** defines the Zookeeper configuration for ClickHouse coordination.
 * Zookeeper is required for distributed ClickHouse deployments to manage replica metadata and leader election.
 */
message KubernetesSignozZookeeperConfig {
  /**
   * Flag to enable or disable Zookeeper deployment.
   * This must be true if ClickHouse clustering is enabled.
   * Defaults to false.
   */
  bool is_enabled = 1;

  // The container specifications for Zookeeper.
  KubernetesSignozZookeeperContainer container = 2 [(default_zookeeper_container) = {
    replicas: 1
    resources: {
      limits: {
        cpu: "500m"
        memory: "512Mi"
      }
      requests: {
        cpu: "100m"
        memory: "256Mi"
      }
    }
    disk_size: "8Gi"
  }];
}

/**
 * **KubernetesSignozZookeeperContainer** specifies the container configuration for Zookeeper.
 */
message KubernetesSignozZookeeperContainer {
  /**
   * The number of Zookeeper pods to deploy.
   * For production, this should be an odd number (3 or 5) to maintain quorum.
   */
  int32 replicas = 1 [(buf.validate.field).int32.gte = 1];

  // The CPU and memory resources allocated to the Zookeeper container.
  org.project_planton.shared.kubernetes.ContainerResources resources = 2;

  // The container image configuration for Zookeeper.
  org.project_planton.shared.kubernetes.ContainerImage image = 3;

  /**
   * The size of the persistent volume attached to each Zookeeper pod (e.g., "8Gi").
   */
  string disk_size = 4 [(buf.validate.field).cel = {
    id: "spec.zookeeper.disk_size.required"
    message: "Disk size is required and must match the format"
    expression: "size(this) > 0 && this.matches('^\\\\d+(\\\\.\\\\d+)?\\\\s?(Ki|Mi|Gi|Ti|Pi|Ei|K|M|G|T|P|E)$')"
  }];
}

/**
 * **KubernetesSignozIngress** defines the ingress configuration for SigNoz components.
 * It provides separate ingress settings for the UI and OpenTelemetry Collector endpoints.
 */
message KubernetesSignozIngress {
  // Ingress configuration for SigNoz UI and API.
  KubernetesSignozIngressEndpoint ui = 1;

  // Ingress configuration for OpenTelemetry Collector data ingestion endpoint.
  KubernetesSignozIngressEndpoint otel_collector = 2;
}

/**
 * **KubernetesSignozIngressEndpoint** defines ingress configuration for a specific SigNoz endpoint.
 */
message KubernetesSignozIngressEndpoint {
  // Flag to enable or disable ingress for this endpoint.
  bool enabled = 1;

  // The full hostname for external access (e.g., "signoz.example.com").
  // This hostname will be configured via Gateway API resources.
  // Required when enabled is true.
  string hostname = 2;

  option (buf.validate.message).cel = {
    id: "spec.ingress.hostname.required"
    expression: "!this.enabled || size(this.hostname) > 0"
    message: "hostname is required when ingress is enabled"
  };
}
